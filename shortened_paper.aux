\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wozniak2018,crafton2019,ernoult2020,bartunov2018,lillicrap2014,bengio2015}
\citation{scellier17}
\citation{hopfield1984}
\providecommand \oddpage@label [2]{}
\oddpage@label{1}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{scellier17}
\citation{mnist1998}
\citation{simonyan2014,srivastava2015tvdn}
\citation{scellier17}
\citation{watts98}
\citation{bartunov2018}
\citation{scellier17}
\citation{davies2018,schemmel2010,shainline2019}
\citation{bullmore2009}
\citation{he2015,srivastava2015}
\citation{xiaohu2011,krishnan2019}
\citation{scellier17}
\citation{scellier17}
\citation{hopfield1984}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.1}}Equilibrium propagation}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:eqp_formulation}{{{2.1}}{2}{Equilibrium propagation}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.1}.1}}Implementation in a continuous Hopfield network}{2}{subsubsection.2.1.1}\protected@file@percent }
\citation{scellier17}
\newlabel{eqn:hardened_sigmoid}{{1}{3}{Implementation in a continuous Hopfield network}{equation.2.1}{}}
\newlabel{eqn:energy}{{2}{3}{Implementation in a continuous Hopfield network}{equation.2.2}{}}
\newlabel{eqn:cost}{{3}{3}{Implementation in a continuous Hopfield network}{equation.2.3}{}}
\newlabel{eqn:total_energy}{{4}{3}{Implementation in a continuous Hopfield network}{equation.2.4}{}}
\newlabel{eqn:dynamics}{{5}{3}{Implementation in a continuous Hopfield network}{equation.2.5}{}}
\citation{glorot2010}
\citation{schmidhuber2015}
\citation{ioffe2015}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\newlabel{eqn:weight_correction}{{7}{4}{Implementation in a continuous Hopfield network}{equation.2.7}{}}
\newlabel{eqn:bias_correction}{{8}{4}{Implementation in a continuous Hopfield network}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.2}}Vanishing gradient problem}{4}{subsection.2.2}\protected@file@percent }
\newlabel{sec:vangrad}{{{2.2}}{4}{Vanishing gradient problem}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{4}{section.3}\protected@file@percent }
\newlabel{sec:implementation}{{3}{4}{Implementation}{section.3}{}}
\citation{scellier17}
\citation{scellier17}
\citation{glorot2010}
\citation{watts98}
\citation{glorot2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.1}}Layered topology with per-layer rates}{5}{subsection.3.1}\protected@file@percent }
\newlabel{sec:basic_topology}{{{3.1}}{5}{Layered topology with per-layer rates}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.2}}Layered topology with global learning rate}{5}{subsection.3.2}\protected@file@percent }
\newlabel{sec:basic_topology_uniform}{{{3.2}}{5}{Layered topology with global learning rate}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.3}}Our topology}{5}{subsection.3.3}\protected@file@percent }
\newlabel{sec:our_topology}{{{3.3}}{5}{Our topology}{subsection.3.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Algorithm to produce our topology\relax }}{5}{algocf.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:ourtop}{{1}{5}{Our topology}{algocf.1}{}}
\citation{mnist1998}
\citation{mnist1998}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.1}}Network performance comparison}{6}{subsection.4.1}\protected@file@percent }
\newlabel{sec:network_performance}{{{4.1}}{6}{Network performance comparison}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.2}}Training rates of individual pairs of layers}{6}{subsection.4.2}\protected@file@percent }
\newlabel{sec:mnist_perlayer}{{{4.2}}{6}{Training rates of individual pairs of layers}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.3}}Effect of $p$}{6}{subsection.4.3}\protected@file@percent }
\newlabel{sec:mnist_1epoch}{{{4.3}}{6}{Effect of $p$}{subsection.4.3}{}}
\citation{indiveri2011,schemmel2010}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{5.1}}Comparing the computational complexity of equilibrium propagation and backpropagation}{7}{subsection.5.1}\protected@file@percent }
\newlabel{sec:comparison}{{{5.1}}{7}{Comparing the computational complexity of equilibrium propagation and backpropagation}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{5.1}.1}}Requirements of equilibrium propagation}{7}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{5.1}.2}}Requirements of backpropagation}{7}{subsubsection.5.1.2}\protected@file@percent }
\citation{ernoult2020}
\citation{lee2015,xie2003,pineda1987}
\citation{lillicrap2014,crafton2019}
\citation{bartunov2018}
\citation{bengio2015}
\citation{shainline2019,davies2018,nahmias2013}
\citation{he2015,srivastava2015,xiaohu2011,krishnan2019}
\citation{ioffe2015,glorot2010}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{5.1}.3}}Comparison}{8}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{5.2}}Related work}{8}{subsection.5.2}\protected@file@percent }
\bibstyle{frontiersinSCNS_ENG_HUMS}
\bibdata{references.bib}
\bibcite{bartunov2018}{{1}{2018}{{Bartunov et~al.}}{{Bartunov, Santoro, Richards, Hinton, and Lillicrap}}}
\bibcite{bengio2015}{{2}{2015}{{Bengio et~al.}}{{Bengio, Lee, Bornschein, and Lin}}}
\bibcite{bullmore2009}{{3}{2009}{{Bullmore and Sporns}}{{}}}
\bibcite{crafton2019}{{4}{2019}{{Crafton et~al.}}{{Crafton, Parihar, Gebhardt, and Raychowdhury}}}
\bibcite{davies2018}{{5}{2018}{{Davies et~al.}}{{Davies, Srinivasa, Lin, Chinya, Joshi, Lines et~al.}}}
\bibcite{ernoult2020}{{6}{2020}{{Ernoult et~al.}}{{Ernoult, Grollier, Querlioz, Bengio, and Scellier}}}
\bibcite{glorot2010}{{7}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{he2015}{{8}{2015}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{hopfield1984}{{9}{1984}{{Hopfield}}{{}}}
\bibcite{indiveri2011}{{10}{2011}{{Indiveri et~al.}}{{Indiveri, Linares-Barranco, Hamilton, van Schaik, Etienne-Cummings, Delbruck et~al.}}}
\bibcite{ioffe2015}{{11}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{krishnan2019}{{12}{2019}{{Krishnan et~al.}}{{Krishnan, Du, and Cao}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{5.3}}Directions for Future Research}{9}{subsection.5.3}\protected@file@percent }
\bibcite{mnist1998}{{13}{1998}{{LeCun and Cortes}}{{}}}
\bibcite{lee2015}{{14}{2015}{{Lee et~al.}}{{Lee, Zhang, Fischer, and Bengio}}}
\bibcite{lillicrap2014}{{15}{2014}{{Lillicrap et~al.}}{{Lillicrap, Cownden, Tweed, and Akerman}}}
\bibcite{nahmias2013}{{16}{2013}{{Nahmias et~al.}}{{Nahmias, Shastri, Tait, and Prucnal}}}
\bibcite{pineda1987}{{17}{1987}{{Pineda}}{{}}}
\bibcite{scellier17}{{18}{2016}{{Scellier and Bengio}}{{}}}
\bibcite{schemmel2010}{{19}{2010}{{Schemmel et~al.}}{{Schemmel, Br{\"u}derle, Gr{\"u}bl, Hock, Meier, and Millner}}}
\bibcite{schmidhuber2015}{{20}{2015}{{Schmidhuber}}{{}}}
\bibcite{shainline2019}{{21}{2019}{{{Shainline} et~al.}}{{{Shainline}, {Buckley}, {McCaughan}, {Chiles}, {Jafari Salim}, {Castellanos-Beltran} et~al.}}}
\bibcite{simonyan2014}{{22}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{srivastava2015}{{23}{2015{a}}{{Srivastava et~al.}}{{Srivastava, Greff, and Schmidhuber}}}
\bibcite{srivastava2015tvdn}{{24}{2015{b}}{{Srivastava et~al.}}{{Srivastava, Greff, and Schmidhuber}}}
\bibcite{watts98}{{25}{1998}{{Watts and Strogatz}}{{}}}
\bibcite{wozniak2018}{{26}{2018}{{Wozniak et~al.}}{{Wozniak, Pantazi, and Eleftheriou}}}
\bibcite{xiaohu2011}{{27}{2011}{{Xiaohu et~al.}}{{Xiaohu, Xiaoling, Jinhua, Yulin, and Maolin}}}
\bibcite{xie2003}{{28}{2003}{{Xie and Seung}}{{}}}
\citation{scellier17}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Topology of the layered network tested in \citep  {scellier17}. All pairs of neurons in adjacent layers are connected. All connections are bidirectional. To compensate for the vanishing gradient problem, the learning rate is reduced by a factor of 4 each time distance from the output decreases by one layer.\relax }}{11}{figure.caption.5}\protected@file@percent }
\newlabel{fig:top_basic}{{1}{11}{Topology of the layered network tested in \citep {scellier17}. All pairs of neurons in adjacent layers are connected. All connections are bidirectional. To compensate for the vanishing gradient problem, the learning rate is reduced by a factor of 4 each time distance from the output decreases by one layer.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Our modifications to the topology of figure \ref  {fig:top_basic} to avoid a vanishing gradient while using a global learning rate. Red dotted lines denote connections that have been removed, black dotted lines denote their replacements, and green solid lines denote added intralayer connections. All connections are bidirectional. This illustration shows a network with $p=8\%$.\relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:top_sw}{{2}{12}{Our modifications to the topology of figure \ref {fig:top_basic} to avoid a vanishing gradient while using a global learning rate. Red dotted lines denote connections that have been removed, black dotted lines denote their replacements, and green solid lines denote added intralayer connections. All connections are bidirectional. This illustration shows a network with $p=8\%$.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance on MNIST of the networks in section \ref  {sec:implementation}. Dashed lines show the test error and solid lines show the training error. In green is a layered network with a global learning rate (section \ref  {sec:basic_topology_uniform}), in orange is a layered network with per-layer rates individually tuned to counter the vanishing gradient problem (section \ref  {sec:basic_topology}), and in green is a network with our topology, $p=7.56\%$ (section \ref  {sec:our_topology}). Observe that our topology is almost as effective as per-layer rates at countering the vanishing gradient problem that impedes training of the layered network with a global learning rate.\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:mnist_comparison}{{3}{13}{Performance on MNIST of the networks in section \ref {sec:implementation}. Dashed lines show the test error and solid lines show the training error. In green is a layered network with a global learning rate (section \ref {sec:basic_topology_uniform}), in orange is a layered network with per-layer rates individually tuned to counter the vanishing gradient problem (section \ref {sec:basic_topology}), and in green is a network with our topology, $p=7.56\%$ (section \ref {sec:our_topology}). Observe that our topology is almost as effective as per-layer rates at countering the vanishing gradient problem that impedes training of the layered network with a global learning rate.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Root-mean-square corrections to weights in different layers while training on MNIST, for the networks in section \ref  {sec:implementation}. For clarity, values were subjected to an 11-point centered moving average. (left) A layered network with a single global learning rate (section \ref  {sec:basic_topology_uniform}). (center) A layered network a unique, individually-tuned learning rate for each layer (section \ref  {sec:basic_topology}). (right) A network with our topology, $p = 7.56\%$ (section \ref  {sec:our_topology}).Observe that the layered topology with a global learning rate has a vanishing gradient problem, which is almost completely solved by tuning an individual learning rate for each layer. Our topology improves the situation by making training uniform among the deeper layers, although the shallowest layer still trains more-quickly than the deeper layers.\relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mnist_layers}{{4}{14}{Root-mean-square corrections to weights in different layers while training on MNIST, for the networks in section \ref {sec:implementation}. For clarity, values were subjected to an 11-point centered moving average. (left) A layered network with a single global learning rate (section \ref {sec:basic_topology_uniform}). (center) A layered network a unique, individually-tuned learning rate for each layer (section \ref {sec:basic_topology}). (right) A network with our topology, $p = 7.56\%$ (section \ref {sec:our_topology}).Observe that the layered topology with a global learning rate has a vanishing gradient problem, which is almost completely solved by tuning an individual learning rate for each layer. Our topology improves the situation by making training uniform among the deeper layers, although the shallowest layer still trains more-quickly than the deeper layers.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Behavior of our network (section \ref  {sec:our_topology}) with varying $p$, during the first epoch of training. (top) The training error after one epoch. (bottom) Root-mean-square correction to weights in different layers during the first epoch. Observe that as $p$ is increased, the error rate decreases and the root-mean-square corrections to each layer become more-uniform.\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{fig:mnist_1epoch}{{5}{15}{Behavior of our network (section \ref {sec:our_topology}) with varying $p$, during the first epoch of training. (top) The training error after one epoch. (bottom) Root-mean-square correction to weights in different layers during the first epoch. Observe that as $p$ is increased, the error rate decreases and the root-mean-square corrections to each layer become more-uniform.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Illustration of the functionality needed to implement equilibrium propagation in hardware. Yellow squares indicate a value that must be stored in memory for a subsequent phase. The circles indicate ($N$) neuron and ($S$) synapse devices with the associated functions described in the text. (a) The functionality required by the neurons and synapses in the free running phase. (b) The functionality of the neurons and synapses (except output neurons) in the weakly clamped phase. (c) The functionality of the neurons and synapses in the weight and bias update phase.\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{fig:eq_prop}{{6}{16}{Illustration of the functionality needed to implement equilibrium propagation in hardware. Yellow squares indicate a value that must be stored in memory for a subsequent phase. The circles indicate ($N$) neuron and ($S$) synapse devices with the associated functions described in the text. (a) The functionality required by the neurons and synapses in the free running phase. (b) The functionality of the neurons and synapses (except output neurons) in the weakly clamped phase. (c) The functionality of the neurons and synapses in the weight and bias update phase.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Illustration of the functionality needed to implement backpropagation in hardware. Yellow squares indicate a value that must be stored in memory for a subsequent phase. The circles indicate ($N$) neuron and ($S$) synapse devices with the associated functions described in the text. (a) The functionality required by the neurons and synapses in the forward pass phase. (b) The functionality of the neurons and synapses (except the last layer of neurons) in the backpropagation phase. (c) The functionality of the neurons and synapses in the weight and bias update phase.\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:back_prop}{{7}{17}{Illustration of the functionality needed to implement backpropagation in hardware. Yellow squares indicate a value that must be stored in memory for a subsequent phase. The circles indicate ($N$) neuron and ($S$) synapse devices with the associated functions described in the text. (a) The functionality required by the neurons and synapses in the forward pass phase. (b) The functionality of the neurons and synapses (except the last layer of neurons) in the backpropagation phase. (c) The functionality of the neurons and synapses in the weight and bias update phase.\relax }{figure.caption.11}{}}
\global\@namedef{@lastpage@}{18}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of the capabilities a hardware neuron would need in order to implement backpropagation and equilibrium propagation.\relax }}{18}{table.caption.13}\protected@file@percent }
\newlabel{table:bp_eqp_contrast}{{1}{18}{Comparison of the capabilities a hardware neuron would need in order to implement backpropagation and equilibrium propagation.\relax }{table.caption.13}{}}

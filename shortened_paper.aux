\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{scellier17}
\citation{hopfield1984}
\citation{scellier17}
\citation{mnist1998}
\citation{simonyan2014,srivastava2015tvdn}
\citation{scellier17}
\citation{watts98}
\citation{bartunov2018}
\citation{scellier17}
\citation{bullmore2009}
\citation{he2015,srivastava2015}
\citation{xiaohu2011,krishnan2019}
\providecommand \oddpage@label [2]{}
\oddpage@label{1}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{scellier17}
\citation{scellier17}
\citation{hopfield1984}
\citation{scellier17}
\citation{bengio2015}
\citation{scellier17}
\citation{scellier17}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.1}}Equilibrium propagation}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:eqp_formulation}{{{2.1}}{2}{Equilibrium propagation}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.1}.1}}Implementation in a continuous Hopfield network}{2}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.2}}Vanishing gradient problem}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec:vangrad}{{{2.2}}{2}{Vanishing gradient problem}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.3}}Implementation}{2}{subsection.2.3}\protected@file@percent }
\newlabel{sec:implementation}{{{2.3}}{2}{Implementation}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.3}.1}}Layered topology with per-layer rates}{2}{subsubsection.2.3.1}\protected@file@percent }
\newlabel{sec:basic_topology}{{{{2.3}.1}}{2}{Layered topology with per-layer rates}{subsubsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.3}.2}}Layered topology with global learning rate}{2}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{sec:basic_topology_uniform}{{{{2.3}.2}}{2}{Layered topology with global learning rate}{subsubsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{2.3}.3}}Our topology}{2}{subsubsection.2.3.3}\protected@file@percent }
\newlabel{sec:our_topology}{{{{2.3}.3}}{2}{Our topology}{subsubsection.2.3.3}{}}
\bibstyle{frontiersinSCNS_ENG_HUMS}
\bibdata{references.bib}
\bibcite{bartunov2018}{{1}{2018}{{Bartunov et~al.}}{{Bartunov, Santoro, Richards, Hinton, and Lillicrap}}}
\bibcite{bengio2015}{{2}{2015}{{Bengio et~al.}}{{Bengio, Lee, Bornschein, and Lin}}}
\bibcite{bullmore2009}{{3}{2009}{{Bullmore and Sporns}}{{}}}
\bibcite{he2015}{{4}{2015}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{hopfield1984}{{5}{1984}{{Hopfield}}{{}}}
\bibcite{krishnan2019}{{6}{2019}{{Krishnan et~al.}}{{Krishnan, Du, and Cao}}}
\bibcite{mnist1998}{{7}{1998}{{LeCun and Cortes}}{{}}}
\bibcite{scellier17}{{8}{2016}{{Scellier and Bengio}}{{}}}
\bibcite{simonyan2014}{{9}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{srivastava2015}{{10}{2015{a}}{{Srivastava et~al.}}{{Srivastava, Greff, and Schmidhuber}}}
\bibcite{srivastava2015tvdn}{{11}{2015{b}}{{Srivastava et~al.}}{{Srivastava, Greff, and Schmidhuber}}}
\bibcite{watts98}{{12}{1998}{{Watts and Strogatz}}{{}}}
\bibcite{xiaohu2011}{{13}{2011}{{Xiaohu et~al.}}{{Xiaohu, Xiaoling, Jinhua, Yulin, and Maolin}}}
\citation{scellier17}
\global\@namedef{@lastpage@}{3}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.1}}Network performance comparison}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:network_performance}{{{3.1}}{3}{Network performance comparison}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.2}}Training rates of individual pairs of layers}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:mnist_perlayer}{{{3.2}}{3}{Training rates of individual pairs of layers}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.3}}Effect of $p$}{3}{subsection.3.3}\protected@file@percent }
\newlabel{sec:mnist_1epoch}{{{3.3}}{3}{Effect of $p$}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.1}}Comparing the computational complexity of equilibrium propagation and backpropagation}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{4.1}.1}}Comparison}{3}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.2}}Related work}{3}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.3}}Directions for Future Research}{3}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Topology of the layered network tested in \citep  {scellier17}. All pairs of neurons in adjacent layers are connected. All connections are bidirectional. To compensate for the vanishing gradient problem, the learning rate is reduced by a factor of 4 each time distance from the output decreases by one layer.\relax }}{4}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:top_basic}{{1}{4}{Topology of the layered network tested in \citep {scellier17}. All pairs of neurons in adjacent layers are connected. All connections are bidirectional. To compensate for the vanishing gradient problem, the learning rate is reduced by a factor of 4 each time distance from the output decreases by one layer.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Our modifications to the topology of figure \ref  {fig:top_basic} to avoid a vanishing gradient while using a global learning rate. Red dotted lines denote connections that have been removed, black dotted lines denote their replacements, and green solid lines denote added intralayer connections. All connections are bidirectional. This illustration shows a network with $p=8\%$.\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:top_sw}{{2}{5}{Our modifications to the topology of figure \ref {fig:top_basic} to avoid a vanishing gradient while using a global learning rate. Red dotted lines denote connections that have been removed, black dotted lines denote their replacements, and green solid lines denote added intralayer connections. All connections are bidirectional. This illustration shows a network with $p=8\%$.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance on MNIST of the networks in section \ref  {sec:implementation}. Dashed lines show the test error and solid lines show the training error. In green is a layered network with a global learning rate (section \ref  {sec:basic_topology_uniform}), in orange is a layered network with per-layer rates individually tuned to counter the vanishing gradient problem (section \ref  {sec:basic_topology}), and in green is a network with our topology, $p=7.56\%$ (section \ref  {sec:our_topology}). Observe that our topology is almost as effective as per-layer rates at countering the vanishing gradient problem that impedes training of the layered network with a global learning rate.\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:mnist_comparison}{{3}{6}{Performance on MNIST of the networks in section \ref {sec:implementation}. Dashed lines show the test error and solid lines show the training error. In green is a layered network with a global learning rate (section \ref {sec:basic_topology_uniform}), in orange is a layered network with per-layer rates individually tuned to counter the vanishing gradient problem (section \ref {sec:basic_topology}), and in green is a network with our topology, $p=7.56\%$ (section \ref {sec:our_topology}). Observe that our topology is almost as effective as per-layer rates at countering the vanishing gradient problem that impedes training of the layered network with a global learning rate.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Root-mean-square corrections to weights in different layers while training on MNIST, for the networks in section \ref  {sec:implementation}. For clarity, values were subjected to an 11-point centered moving average. (top left) A layered network with a single global learning rate (section \ref  {sec:basic_topology_uniform}). (top center) A layered network a unique, individually-tuned learning rate for each layer (section \ref  {sec:basic_topology}). (top right) A network with our topology, $p = 7.56\%$ (section \ref  {sec:our_topology}). (bottom left) A layered network with 5 100-neuron hidden layers and a single global learning rate (section \ref  {sec:deep_topology}). (bottom right) A network with our topology, $p=7.56\%$, and 5 100-neuron hidden layers (section \ref  {sec:deep_topology}). Observe that for the shallower networks the layered topology with a global learning rate has a vanishing gradient problem, which is almost completely solved by tuning an individual learning rate for each layer. Our topology improves the situation by making training uniform among the deeper layers, although the shallowest layer still trains more-quickly than the deeper layers. For the deeper networks, the same trend is apparent but not as strong; we believe this is due to sub-optimal hyperparameter settings.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:mnist_layers}{{4}{7}{Root-mean-square corrections to weights in different layers while training on MNIST, for the networks in section \ref {sec:implementation}. For clarity, values were subjected to an 11-point centered moving average. (top left) A layered network with a single global learning rate (section \ref {sec:basic_topology_uniform}). (top center) A layered network a unique, individually-tuned learning rate for each layer (section \ref {sec:basic_topology}). (top right) A network with our topology, $p = 7.56\%$ (section \ref {sec:our_topology}). (bottom left) A layered network with 5 100-neuron hidden layers and a single global learning rate (section \ref {sec:deep_topology}). (bottom right) A network with our topology, $p=7.56\%$, and 5 100-neuron hidden layers (section \ref {sec:deep_topology}). Observe that for the shallower networks the layered topology with a global learning rate has a vanishing gradient problem, which is almost completely solved by tuning an individual learning rate for each layer. Our topology improves the situation by making training uniform among the deeper layers, although the shallowest layer still trains more-quickly than the deeper layers. For the deeper networks, the same trend is apparent but not as strong; we believe this is due to sub-optimal hyperparameter settings.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Behavior of our network (section \ref  {sec:our_topology}) with varying $p$, during the first epoch of training. (top) The training error after one epoch. (bottom) Root-mean-square correction to weights in different layers during the first epoch. Observe that as $p$ is increased, the error rate decreases and the root-mean-square corrections to each layer become more-uniform.\relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mnist_1epoch}{{5}{8}{Behavior of our network (section \ref {sec:our_topology}) with varying $p$, during the first epoch of training. (top) The training error after one epoch. (bottom) Root-mean-square correction to weights in different layers during the first epoch. Observe that as $p$ is increased, the error rate decreases and the root-mean-square corrections to each layer become more-uniform.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Illustration of the functionality needed to implement equilibrium propagation in hardware. Black lines denote functionality needed in the free phase. Green lines denote functionality to correct parameters. Orange lines denote functionality needed only by output neurons, that is unique to the weakly-clamped phase. There is no functionality unique to the weakly-clamped phase that is needed by all neurons.\relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig:eqp_bd}{{6}{9}{Illustration of the functionality needed to implement equilibrium propagation in hardware. Black lines denote functionality needed in the free phase. Green lines denote functionality to correct parameters. Orange lines denote functionality needed only by output neurons, that is unique to the weakly-clamped phase. There is no functionality unique to the weakly-clamped phase that is needed by all neurons.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Illustration of the functionality needed to implement backpropagation in hardware. Black lines denote functionality needed in the forwards phase. Green lines denote functionality to correct parameters. Red lines denote functionality unique to the backwards phase that is needed by all neurons. Orange lines denote functionality needed only by output neurons, that is unique to the backwards phase.\relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{fig:backprop_bd}{{7}{10}{Illustration of the functionality needed to implement backpropagation in hardware. Black lines denote functionality needed in the forwards phase. Green lines denote functionality to correct parameters. Red lines denote functionality unique to the backwards phase that is needed by all neurons. Orange lines denote functionality needed only by output neurons, that is unique to the backwards phase.\relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax }}{11}{table.caption.12}\protected@file@percent }
\newlabel{table:bp_eqp_contrast}{{1}{11}{\relax }{table.caption.12}{}}

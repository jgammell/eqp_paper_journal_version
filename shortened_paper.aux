\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{scellier17}
\citation{hopfield1984}
\citation{scellier17}
\citation{mnist1998}
\citation{simonyan2014,srivastava2015tvdn}
\citation{scellier17}
\citation{watts98}
\citation{bartunov2018}
\citation{scellier17}
\citation{bullmore2009}
\citation{he2015,srivastava2015}
\citation{xiaohu2011,krishnan2019}
\citation{scellier17}
\citation{scellier17}
\citation{hopfield1984}
\citation{scellier17}
\citation{bengio2015}
\citation{scellier17}
\citation{scellier17}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Theory}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Equilibrium propagation}{1}{subsection.2.1}\protected@file@percent }
\newlabel{sec:eqp_formulation}{{2.1}{1}{Equilibrium propagation}{subsection.2.1}{}}
\citation{glorot2010}
\citation{schmidhuber2015}
\citation{ioffe2015}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\citation{scellier17}
\citation{scellier17}
\citation{glorot2010}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Topology of the layered network tested in \citep  {scellier17}. All pairs of neurons in adjacent layers are connected. All connections are bidirectional. To compensate for the vanishing gradient problem, the learning rate is reduced by a factor of 4 each time distance from the output decreases by one layer.\relax }}{2}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:top_basic}{{1}{2}{Topology of the layered network tested in \cite {scellier17}. All pairs of neurons in adjacent layers are connected. All connections are bidirectional. To compensate for the vanishing gradient problem, the learning rate is reduced by a factor of 4 each time distance from the output decreases by one layer.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Vanishing gradient problem}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec:vangrad}{{2.2}{2}{Vanishing gradient problem}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{2}{section.3}\protected@file@percent }
\newlabel{sec:implementation}{{3}{2}{Implementation}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Our modifications to the topology of figure \ref  {fig:top_basic} to avoid a vanishing gradient while using a global learning rate. Red dotted lines denote connections that have been removed, black dotted lines denote their replacements, and green solid lines denote added intralayer connections. All connections are bidirectional. This illustration shows a network with $p=8\%$.\relax }}{2}{figure.caption.6}\protected@file@percent }
\newlabel{fig:top_sw}{{2}{2}{Our modifications to the topology of figure \ref {fig:top_basic} to avoid a vanishing gradient while using a global learning rate. Red dotted lines denote connections that have been removed, black dotted lines denote their replacements, and green solid lines denote added intralayer connections. All connections are bidirectional. This illustration shows a network with $p=8\%$.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Layered topology with per-layer rates}{2}{subsection.3.1}\protected@file@percent }
\newlabel{sec:basic_topology}{{3.1}{2}{Layered topology with per-layer rates}{subsection.3.1}{}}
\citation{watts98}
\citation{glorot2010}
\citation{mnist1998}
\citation{mnist1998}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Layered topology with global learning rate}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:basic_topology_uniform}{{3.2}{3}{Layered topology with global learning rate}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Our topology}{3}{subsection.3.3}\protected@file@percent }
\newlabel{sec:our_topology}{{3.3}{3}{Our topology}{subsection.3.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Algorithm to produce our topology\relax }}{3}{algocf.1}\protected@file@percent }
\newlabel{alg:ourtop}{{1}{3}{Our topology}{algocf.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Network performance comparison}{3}{subsection.4.1}\protected@file@percent }
\newlabel{sec:network_performance}{{4.1}{3}{Network performance comparison}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance on MNIST of the networks in section \ref  {sec:implementation}. Dashed lines show the test error and solid lines show the training error. In green is a layered network with a global learning rate (section \ref  {sec:basic_topology_uniform}), in orange is a layered network with per-layer rates individually tuned to counter the vanishing gradient problem (section \ref  {sec:basic_topology}), and in green is a network with our topology, $p=7.56\%$ (section \ref  {sec:our_topology}). Observe that our topology is almost as effective as per-layer rates at countering the vanishing gradient problem that impedes training of the layered network with a global learning rate.\relax }}{3}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mnist_comparison}{{3}{3}{Performance on MNIST of the networks in section \ref {sec:implementation}. Dashed lines show the test error and solid lines show the training error. In green is a layered network with a global learning rate (section \ref {sec:basic_topology_uniform}), in orange is a layered network with per-layer rates individually tuned to counter the vanishing gradient problem (section \ref {sec:basic_topology}), and in green is a network with our topology, $p=7.56\%$ (section \ref {sec:our_topology}). Observe that our topology is almost as effective as per-layer rates at countering the vanishing gradient problem that impedes training of the layered network with a global learning rate.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training rates of individual pairs of layers}{3}{subsection.4.2}\protected@file@percent }
\newlabel{sec:mnist_perlayer}{{4.2}{3}{Training rates of individual pairs of layers}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Effect of $p$}{3}{subsection.4.3}\protected@file@percent }
\citation{lee2015,xie2003,pineda1987}
\citation{lillicrap2014,@crafton2019}
\citation{bartunov2018}
\citation{bengio2015}
\citation{shainline2019,davies2018,nahmias2013}
\citation{he2015,srivastava2015,xiaohu2011,krishnan2019}
\citation{ioffe2015,glorot2010}
\bibstyle{plain}
\bibdata{references}
\bibcite{bartunov2018}{{1}{}{{}}{{}}}
\bibcite{bengio2015}{{2}{}{{}}{{}}}
\bibcite{bullmore2009}{{3}{}{{}}{{}}}
\bibcite{@crafton2019}{{4}{}{{}}{{}}}
\bibcite{davies2018}{{5}{}{{}}{{}}}
\bibcite{glorot2010}{{6}{}{{}}{{}}}
\bibcite{he2015}{{7}{}{{}}{{}}}
\bibcite{hopfield1984}{{8}{}{{}}{{}}}
\bibcite{ioffe2015}{{9}{}{{}}{{}}}
\bibcite{krishnan2019}{{10}{}{{}}{{}}}
\bibcite{mnist1998}{{11}{}{{}}{{}}}
\bibcite{lee2015}{{12}{}{{}}{{}}}
\bibcite{lillicrap2014}{{13}{}{{}}{{}}}
\bibcite{nahmias2013}{{14}{}{{}}{{}}}
\bibcite{pineda1987}{{15}{}{{}}{{}}}
\bibcite{scellier17}{{16}{}{{}}{{}}}
\bibcite{schmidhuber2015}{{17}{}{{}}{{}}}
\bibcite{shainline2019}{{18}{}{{}}{{}}}
\bibcite{simonyan2014}{{19}{}{{}}{{}}}
\bibcite{srivastava2015tvdn}{{20}{}{{}}{{}}}
\bibcite{srivastava2015}{{21}{}{{}}{{}}}
\bibcite{watts98}{{22}{}{{}}{{}}}
\bibcite{xiaohu2011}{{23}{}{{}}{{}}}
\bibcite{xie2003}{{24}{}{{}}{{}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.185pt}
\newlabel{tocindent2}{10.34999pt}
\newlabel{tocindent3}{0pt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Root-mean-square corrections to weights in different layers while training on MNIST, for the networks in section \ref  {sec:implementation}. For clarity, values were subjected to an 11-point centered moving average. (left) A layered network with a single global learning rate (section \ref  {sec:basic_topology_uniform}). (center) A layered network a unique, individually-tuned learning rate for each layer (section \ref  {sec:basic_topology}). (right) A network with our topology, $p = 7.56\%$ (section \ref  {sec:our_topology}). Observe that the layered topology with a global learning rate has a vanishing gradient problem, which is almost completely solved by tuning an individual learning rate for each layer. Our topology improves the situation by making training uniform among the deeper layers, although the shallowest layer still trains more-quickly than the deeper layers.\relax }}{4}{figure.caption.9}\protected@file@percent }
\newlabel{fig:mnist_layers}{{4}{4}{Root-mean-square corrections to weights in different layers while training on MNIST, for the networks in section \ref {sec:implementation}. For clarity, values were subjected to an 11-point centered moving average. (left) A layered network with a single global learning rate (section \ref {sec:basic_topology_uniform}). (center) A layered network a unique, individually-tuned learning rate for each layer (section \ref {sec:basic_topology}). (right) A network with our topology, $p = 7.56\%$ (section \ref {sec:our_topology}). Observe that the layered topology with a global learning rate has a vanishing gradient problem, which is almost completely solved by tuning an individual learning rate for each layer. Our topology improves the situation by making training uniform among the deeper layers, although the shallowest layer still trains more-quickly than the deeper layers.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Related work}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Directions for Future Research}{4}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{4}{section*.11}\protected@file@percent }
\newlabel{TotPages}{{4}{4}{}{page.4}{}}

% very deep convolutional networks for large-scale image recognition
% training very deep networks

@article{crafton2019,
  author    = {Brian Crafton and
               Abhinav Parihar and
               Evan Gebhardt and
               Arijit Raychowdhury},
  title     = {Direct Feedback Alignment with Sparse Connections for Local Learning},
  journal   = {CoRR},
  volume    = {abs/1903.02083},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.02083},
  archivePrefix = {arXiv},
  eprint    = {1903.02083},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-02083.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{simonyan2014,
    title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
    author={Karen Simonyan and Andrew Zisserman},
    year={2014},
    eprint={1409.1556},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{srivastava2015tvdn,
    title={Training Very Deep Networks},
    author={Rupesh Kumar Srivastava and Klaus Greff and Jürgen Schmidhuber},
    year={2015},
    eprint={1507.06228},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{hopfield1984,
author = {Hopfield, John},
year = {1984},
month = {06},
pages = {3088-92},
title = {Neurons With Graded Response Have Collective Computational Properties Like Those of Two-State Neurons},
volume = {81},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
doi = {10.1073/pnas.81.10.3088}
}
% continuous Hopfield network


@article{scellier17,
    title={Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation},
    author={Benjamin Scellier and Yoshua Bengio},
    year={2016},
    eprint={1602.05179},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
% Scellier's paper on equilibrium propagation

@misc{mnist1998,
         author = "Y. LeCun and C. Cortes",
         title  = "The mnist database of handwritten digits",
         year   = "1998"
     }

@article{watts98,
         author  = "D. Watts and S. Strogatz",
         title   = "Collective dynamics of 'small-world' networks",
         journal = "Nature",
         year    = "1998"
        }
% The original paper describing small-world networks

@article{bullmore2009,
         author  = "E. Bullmore and O. Sporns",
         title   = "Complex brain networks: graph theoretical analysis of structural and functional systems",
         journal = "Nature",
         year    = "2009"
        }
% Asserts that small-world networks are present in biological brains

@inproceedings{xiaohu2011,
               author    = "L. Xiaohu and L. Xiaoling and Z. Jinhua and Z. Yulin and L. Maolin",
               title     = "A new multilayer feedforward small-world neural network with its performances on function approximation",
               booktitle = "2011 IEEE International Conference on Computer Science and Automation Engineering",
               year      = "2011"
              }
% An example of small-world MLFFNN trained through backpropagation

@article{krishnan2019,
    title={Structural Pruning in Deep Neural Networks: A Small-World Approach},
    author={Gokul Krishnan and Xiaocong Du and Yu Cao},
    year={2019},
    eprint={1911.04453},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
% An example of a small-world deep convolutional network

@InProceedings{glorot2010,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Xavier Glorot and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Yee Whye Teh and Mike Titterington},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}
% Describes weight initialization scheme used by Scellier

@article{bengio2015,
  author    = {Yoshua Bengio and
               Dong{-}Hyun Lee and
               J{\"{o}}rg Bornschein and
               Zhouhan Lin},
  title     = {Towards Biologically Plausible Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1502.04156},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.04156},
  archivePrefix = {arXiv},
  eprint    = {1502.04156},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BengioLBL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Talks about biological implausibility of backpropagation, and leaky integrator neurons

@article{bartunov2018,
  author    = {Sergey Bartunov and
               Adam Santoro and
               Blake A. Richards and
               Geoffrey E. Hinton and
               Timothy P. Lillicrap},
  title     = {Assessing the Scalability of Biologically-Motivated Deep Learning
               Algorithms and Architectures},
  journal   = {CoRR},
  volume    = {abs/1807.04587},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.04587},
  archivePrefix = {arXiv},
  eprint    = {1807.04587},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-04587.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Overview of biologically-motivated approaches and their effectiveness on hard datasets

@article{lillicrap2014,
    title={Random feedback weights support learning in deep neural networks},
    author={Timothy P. Lillicrap and Daniel Cownden and Douglas B. Tweed and Colin J. Akerman},
    year={2014},
    eprint={1411.0247},
    archivePrefix={arXiv},
    primaryClass={q-bio.NC}
}
% Lillicrap's paper that shows feedback connections will approach feedforward weights if not fixed

@inproceedings{lee2015,
author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Y.},
year = {2015},
month = {08},
pages = {498-515},
title = {Difference Target Propagation},
isbn = {978-3-319-23527-1},
doi = {10.1007/978-3-319-23528-8_31}
}
% Difference target propagation paper

@article{xie2003,
author = {Xie, Xiaohui and Seung, Hyunjune},
year = {2003},
month = {03},
pages = {441-54},
title = {Equivalence of Backpropagation and Contrastive Hebbian Learning in a Layered Network},
volume = {15},
journal = {Neural computation},
doi = {10.1162/089976603762552988}
}
% Contrastive Hebbian learning

@article{pineda1987,
title = "Generalization of back-propagation to recurrent neural networks",
author = "Pineda, {Fernando J}",
year = "1987",
volume = "59",
pages = "2229--2232",
journal = "Physical Review Letters",
issn = "0031-9007",
publisher = "American Physical Society",
number = "19",
}

% Recurrent backpropagation

@inproceedings{oconnor2018,
  title={Training a Network of Spiking Neurons with Equilibrium Propagation},
  author={Peter O'Connor and Efstratios Gavves and Max Welling},
  year={2018}
}

%Spiking EQP network

@article{he2015,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% ResNet paper

@ARTICLE{shainline2019,
       author = {{Shainline}, Jeffrey M. and {Buckley}, Sonia M. and
         {McCaughan}, Adam N. and {Chiles}, Jeffrey T. and {Jafari Salim}, Amir and
         {Castellanos-Beltran}, Manuel and {Donnelly}, Christine A. and
         {Schneider}, Michael L. and {Mirin}, Richard P. and {Nam}, Sae Woo},
        title = "{Superconducting optoelectronic loop neurons}",
      journal = {Journal of Applied Physics},
         year = "2019",
        month = "Jul",
       volume = {126},
       number = {4},
          eid = {044902},
        pages = {044902},
          doi = {10.1063/1.5096403},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019JAP...126d4902S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% Our group's paper - maybe applicable to analog implementation

@article{srivastava2015,
  author    = {Rupesh Kumar Srivastava and
               Klaus Greff and
               J{\"{u}}rgen Schmidhuber},
  title     = {Highway Networks},
  journal   = {CoRR},
  volume    = {abs/1505.00387},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.00387},
  archivePrefix = {arXiv},
  eprint    = {1505.00387},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SrivastavaGS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Highway networks paper

@article{ioffe2015,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/1502.03167},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03167},
  archivePrefix = {arXiv},
  eprint    = {1502.03167},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/IoffeS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Batch normalization paper

@article{davies2018,
author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Joshi, Prasad and Lines, Andrew and Wild, Andreas and Wang, Hong},
year = {2018},
month = {01},
pages = {1-1},
title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
volume = {PP},
journal = {IEEE Micro},
doi = {10.1109/MM.2018.112130359}
}

% related to analog implementation

@article{nahmias2013,
author = {Nahmias, Mitchell and Shastri, Bhavin and Tait, A.N. and Prucnal, P.R.},
year = {2013},
month = {09},
pages = {1-12},
title = {A Leaky Integrate-and-Fire Laser Neuron for Ultrafast Cognitive Computing},
volume = {19},
journal = {Selected Topics in Quantum Electronics, IEEE Journal of},
doi = {10.1109/JSTQE.2013.2257700}
}

% related to analog implementation

@article{humphries2008,
author = {Humphries, Mark and Gurney, Kevin},
year = {2008},
month = {02},
pages = {e0002051},
title = {Network ‘Small-World-Ness’: A Quantitative Method for Determining Canonical Network Equivalence},
volume = {3},
journal = {PloS one},
doi = {10.1371/journal.pone.0002051}
}
% Talks about small-world coefficient

@article{schmidhuber2015,
   title={Deep learning in neural networks: An overview},
   volume={61},
   ISSN={0893-6080},
   url={http://dx.doi.org/10.1016/j.neunet.2014.09.003},
   DOI={10.1016/j.neunet.2014.09.003},
   journal={Neural Networks},
   publisher={Elsevier BV},
   author={Schmidhuber, Jürgen},
   year={2015},
   month={Jan},
   pages={85–117}
}

@ARTICLE{indiveri2011,
  
AUTHOR={Indiveri, Giacomo and Linares-Barranco, Bernabe and Hamilton, Tara and van Schaik, André and Etienne-Cummings, Ralph and Delbruck, Tobi and Liu, Shih-Chii and Dudek, Piotr and Häfliger, Philipp and Renaud, Sylvie and Schemmel, Johannes and Cauwenberghs, Gert and Arthur, John and Hynna, Kai and Folowosele, Fopefolu and SAÏGHI, Sylvain and Serrano-Gotarredona, Teresa and Wijekoon, Jayawan and Wang, Yingxue and Boahen, Kwabena},   
	 
TITLE={Neuromorphic Silicon Neuron Circuits},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={5},      

PAGES={73},     
	
YEAR={2011},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnins.2011.00073},       
	
DOI={10.3389/fnins.2011.00073},      
	
ISSN={1662-453X},   
   
ABSTRACT={Hardware implementations of spiking neurons can be extremely useful for a large variety of applications, ranging from high-speed modeling of large-scale neural systems to real-time behaving systems, to bidirectional brain–machine interfaces. The specific circuit solutions used to implement silicon neurons depend on the application requirements. In this paper we describe the most common building blocks and techniques used to implement these circuits, and present an overview of a wide range of neuromorphic silicon neurons, which implement different computational models, ranging from biophysically realistic and conductance-based Hodgkin–Huxley models to bi-dimensional generalized adaptive integrate and fire models. We compare the different design methodologies used for each silicon neuron design described, and demonstrate their features with experimental results, measured from a wide range of fabricated VLSI chips.}
}

% Usefulness of biologically-inspired learning
@article{wozniak2018,
  author    = {Stanislaw Wozniak and
               Angeliki Pantazi and
               Evangelos Eleftheriou},
  title     = {Deep Networks Incorporating Spiking Neural Dynamics},
  journal   = {CoRR},
  volume    = {abs/1812.07040},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.07040},
  archivePrefix = {arXiv},
  eprint    = {1812.07040},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-07040.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Continuous EQP
@article{ernoult2020,
      title={Equilibrium Propagation with Continual Weight Updates}, 
      author={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},
      year={2020},
      eprint={2005.04168},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@article{schemmel2010,
  title={A wafer-scale neuromorphic hardware system for large-scale neural modeling},
  author={J. Schemmel and Daniel Br{\"u}derle and A. Gr{\"u}bl and Matthias Hock and K. Meier and S. Millner},
  journal={Proceedings of 2010 IEEE International Symposium on Circuits and Systems},
  year={2010},
  pages={1947-1950}
}

% Fashion MNIST
@online{fmnist2017,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

% Scikit Learn
@article{sklearn2011,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

% Pytorch
@incollection{pytorch2019,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@ARTICLE{pedroni2019,
  
AUTHOR={Pedroni, Bruno U. and Joshi, Siddharth and Deiss, Stephen R. and Sheik, Sadique and Detorakis, Georgios and Paul, Somnath and Augustine, Charles and Neftci, Emre O. and Cauwenberghs, Gert},   
	 
TITLE={Memory-Efficient Synaptic Connectivity for Spike-Timing- Dependent Plasticity},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={13},      

PAGES={357},     
	
YEAR={2019},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnins.2019.00357},       
	
DOI={10.3389/fnins.2019.00357},      
	
ISSN={1662-453X},   
   
ABSTRACT={Spike-Timing-Dependent Plasticity (STDP) is a bio-inspired local incremental weight update rule commonly used for online learning in spike-based neuromorphic systems. In STDP, the intensity of long-term potentiation and depression in synaptic efficacy (weight) between neurons is expressed as a function of the relative timing between pre- and post-synaptic action potentials (spikes), while the polarity of change is dependent on the order (causality) of the spikes. Online STDP weight updates for causal and acausal relative spike times are activated at the onset of post- and pre-synaptic spike events, respectively, implying access to synaptic connectivity both in forward (pre-to-post) and reverse (post-to-pre) directions. Here we study the impact of different arrangements of synaptic connectivity tables on weight storage and STDP updates for large-scale neuromorphic systems. We analyze the memory efficiency for varying degrees of density in synaptic connectivity, ranging from crossbar arrays for full connectivity to pointer-based lookup for sparse connectivity. The study includes comparison of storage and access costs and efficiencies for each memory arrangement, along with a trade-off analysis of the benefits of each data structure depending on application requirements and budget. Finally, we present an alternative formulation of STDP via a delayed causal update mechanism that permits efficient weight access, requiring no more than forward connectivity lookup. We show functional equivalence of the delayed causal updates to the original STDP formulation, with substantial savings in storage and access costs and efficiencies for networks with sparse synaptic connectivity as typically encountered in large-scale models in computational neuroscience.}
}

